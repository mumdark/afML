# 二、自动微分

```py
import torch
```


```py
#创建张量grad跟踪其相关的计算
x = torch.rand( 2, 2, requires_grad=True )
print(x)
#tensor([[0.5804, 0.7581],
#        [0.3407, 0.0812]], requires_grad=True)
```

```py
#做个加法操作
y = x+2
y
#tensor([[2.5804, 2.7581],
#        [2.3407, 2.0812]], grad_fn=<AddBackward0>)
```

```py
#y 作为操作的结果被创建，所以它有 grad_fn
y.grad_fn
#<AddBackward0 at 0x223811e8760>

z = y * y * 3
z
#tensor([[19.9751, 22.8209],
#        [16.4365, 12.9941]], grad_fn=<MulBackward0>)

out = z.mean()
out
#tensor(18.0566, grad_fn=<MeanBackward0>)
```


# 一些解释及举例


**自动微分（Automatic Differentiation），也称为自动求导，是一种用于计算函数导数的技术。在机器学习和深度学习中，我们经常需要计算函数相对于其输入的导数（梯度），以便优化模型参数或进行反向传播算法。**


**传统的微分计算涉及手动推导和编写导数计算的代码，这对于复杂的函数和深度学习模型来说非常繁琐和容易出错。而自动微分则通过计算机程序自动地计算导数，避免了手动推导和编写导数计算代码的麻烦。**


**自动微分的核心思想是通过构建计算图来跟踪函数的执行过程，并在图中记录每个操作的导数计算方式。计算图是由张量（Tensors）和函数（Functions）组成的，张量是输入和输出数据的容器，函数表示对张量进行的操作。**


**在自动微分中，我们首先将输入数据放入张量中，并通过一系列的张量操作构建计算图。然后，执行前向传播，计算函数的输出。在这个过程中，计算图记录了函数的计算过程和操作之间的依赖关系。**


**在前向传播的同时，计算图还会存储每个操作的导数计算方式。这样，在执行反向传播时，可以利用链式法则，从输出节点开始，逐层计算每个节点相对于它的输入节点的导数。这样，我们可以有效地传播梯度并计算导数。**


**通过自动微分，我们可以轻松地计算复杂函数的导数，无需手动推导或编写导数计算的代码。这为深度学习中的模型训练和优化提供了便捷的方式，同时也支持了反向传播算法在神经网络中的应用，从而实现了高效的参数更新和优化。**



**假设我们有一个复杂的函数：**


- f(x) = (sin(x) + x^2) / log(x)


- 这个函数包含了三个不同的数学运算：正弦函数sin(x)，平方运算x^2和对数运算log(x)。计算这个函数的导数可以非常繁琐，特别是在计算更高阶导数时。使用自动微分可以轻松地计算该函数的导数，而无需手动推导或编写导数计算的代码。

让我们使用PyTorch来演示如何使用自动微分计算该函数的导数：


```py
import torch

# 定义复杂函数 f(x)
def f(x):
    return (torch.sin(x) + x**2) / torch.log(x)

# 创建一个输入张量 x，并告知 PyTorch 需要计算关于 x 的梯度
x = torch.tensor(2.0, requires_grad=True)

# 前向传播，计算函数输出
output = f(x)

# 自动计算导数，执行反向传播
output.backward()

# 得到 x 的导数，存储在 x.grad 属性中
print(x.grad)
#tensor(1.4236)
```

**这个结果表示在x=2.0的位置，函数f(x)的导数是约1.4236。通过自动微分，我们可以轻松地计算复杂函数f(x)的导数，无需手动推导或编写导数计算的代码。这为深度学习中复杂模型的训练和优化提供了便捷的方式。**

**知道了x=2.0时的导数有很多用途，特别是在优化问题和机器学习中。这里列举了一些重要的用途：**

1. 参数优化：在机器学习中，我们通常使用梯度下降等优化算法来最小化损失函数。损失函数相对于模型参数的导数告诉我们在当前参数值下，通过微调参数的方向和速度，可以使损失函数下降最快。在x=2.0的位置得到的导数值1.4236，可以指导优化算法调整参数使损失函数下降，从而优化模型。

2. 模型训练：在深度学习中，通过反向传播算法，我们可以计算模型中所有参数的导数，从而对模型进行训练。在每次迭代中，计算x=2.0时的导数，可以帮助我们更新参数，使得模型逐步逼近最优解，从而提高模型的性能。

3. 灵敏度分析：导数告诉我们在x=2.0附近，函数f(x)对x的变化有多敏感。较大的导数值表示函数对x的变化非常敏感，较小的导数值表示函数对x的变化不敏感。这对于理解函数的行为和特性非常有帮助。

4. 曲线拟合：通过计算导数，我们可以估计函数在不同点的局部斜率，从而可以更好地了解函数在不同区域的趋势和形状。这对于拟合复杂的数据曲线非常有用。

5. 极值点分析：导数可以帮助我们分析函数的极值点。在x=2.0的位置得到的导数值为正数，表示函数在x=2.0的位置是增加的，这意味着在附近可能存在一个局部极小值点。